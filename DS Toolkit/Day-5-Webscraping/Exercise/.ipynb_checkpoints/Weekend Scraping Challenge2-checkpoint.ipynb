{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm_notebook as tqdm # this is a fancy progress bar!\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Data Challenge: Scraping Jobs.ch\n",
    "\n",
    "(Weekend homework recap)\n",
    "\n",
    "As a job seeker, one has to search through job portals to find most relevant jobs related to your profile.\n",
    "\n",
    "In this challenge, your goal is to find all jobs related to keywords: “Data Scientist”, “Data Analyst”, “Python Developer”, “Data Engineer”, “Data Manager”, “Data Architect”, “Big Data Analyst” and “Data Python” on jobs.ch.\n",
    "\n",
    "## Questions\n",
    "\n",
    "Download all necessary information (including job text, job rank, company name, job keyword…) for all webpages.\n",
    "Using the information obtained, perform a descriptive analysis on this data including questions:\n",
    "\n",
    "1. How many jobs are shared between these categories?\n",
    "2. How much the keywords: “Data Analyst” and “Big Data Analyst” overlap?\n",
    "3. Are there some companies doing more hires than average?\n",
    "4. How many jobs are there in different Kantons?\n",
    "5. Is “machine learning” keyword most often in data scientist or data analyst jobs?\n",
    "6. What is the distribution of most common keywords between and across categories?\n",
    "7. Produce a report in the form of a clean notebook (or jupyter slides), with commented code and markdown cells for structuring and interpretations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA SCIENTIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617e904698f848878c93087134078717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec9445a2fa443339235246ced104bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=457), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>content</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-16 17:51:46.116394</td>\n",
       "      <td>Swiss Life Asset Managers</td>\n",
       "      <td>Market Data Analyst</td>\n",
       "      <td>24.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanySwiss Life Asset Man...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-16 17:51:48.352983</td>\n",
       "      <td>PwC</td>\n",
       "      <td>Data Scientist (Consultant)</td>\n",
       "      <td>28.10.2019</td>\n",
       "      <td>Job descriptionInfoCompany Du arbeitest bei in...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-16 17:51:50.351856</td>\n",
       "      <td>Die Schweizerische Post</td>\n",
       "      <td>Data Analyst Prozessentwicklung CRM Kundendien...</td>\n",
       "      <td>13.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyData Analyst Prozess...</td>\n",
       "      <td>Bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-16 17:51:52.717220</td>\n",
       "      <td>EY</td>\n",
       "      <td>Senior Data Scientist - Forensics in Zurich</td>\n",
       "      <td>01.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanySenior Data Scientis...</td>\n",
       "      <td>Zurich, CH-ZH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-16 17:51:54.868550</td>\n",
       "      <td>Geberit AG</td>\n",
       "      <td>Product Data Analyst (m/w)</td>\n",
       "      <td>15.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyProduct Data Analyst...</td>\n",
       "      <td>Rapperswil-Jona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>2019-11-16 18:07:56.499864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Senior ETL Informatica and Oracle Developer - ...</td>\n",
       "      <td>11.07.2019</td>\n",
       "      <td>Job descriptionInfoSenior ETL Informatica and ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>2019-11-16 18:07:58.868150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Senior Business Analyst Regulatory &amp; Complianc...</td>\n",
       "      <td>14.10.2019</td>\n",
       "      <td>Job descriptionInfoSenior Business Analyst Reg...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>2019-11-16 18:08:00.999260</td>\n",
       "      <td>PwC</td>\n",
       "      <td>Senior Consultant / Manager Data Analytics (Ad...</td>\n",
       "      <td>05.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanySenior Consultant / ...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>2019-11-16 18:08:03.173268</td>\n",
       "      <td>Credit Suisse AG</td>\n",
       "      <td>Data Sourcing, Integration &amp; Transformation</td>\n",
       "      <td>22.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyData Sourcing, Integ...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>2019-11-16 18:08:05.394270</td>\n",
       "      <td>La Prairie Group AG</td>\n",
       "      <td>Global Digital Media Manager</td>\n",
       "      <td>01.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyGlobal Digital Media...</td>\n",
       "      <td>Volketswil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>457 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date                    company  \\\n",
       "0   2019-11-16 17:51:46.116394  Swiss Life Asset Managers   \n",
       "1   2019-11-16 17:51:48.352983                        PwC   \n",
       "2   2019-11-16 17:51:50.351856    Die Schweizerische Post   \n",
       "3   2019-11-16 17:51:52.717220                         EY   \n",
       "4   2019-11-16 17:51:54.868550                 Geberit AG   \n",
       "..                         ...                        ...   \n",
       "452 2019-11-16 18:07:56.499864                        NaN   \n",
       "453 2019-11-16 18:07:58.868150                        NaN   \n",
       "454 2019-11-16 18:08:00.999260                        PwC   \n",
       "455 2019-11-16 18:08:03.173268           Credit Suisse AG   \n",
       "456 2019-11-16 18:08:05.394270        La Prairie Group AG   \n",
       "\n",
       "                                                 title   published  \\\n",
       "0                                  Market Data Analyst  24.10.2019   \n",
       "1                          Data Scientist (Consultant)  28.10.2019   \n",
       "2    Data Analyst Prozessentwicklung CRM Kundendien...  13.11.2019   \n",
       "3          Senior Data Scientist - Forensics in Zurich  01.11.2019   \n",
       "4                           Product Data Analyst (m/w)  15.11.2019   \n",
       "..                                                 ...         ...   \n",
       "452  Senior ETL Informatica and Oracle Developer - ...  11.07.2019   \n",
       "453  Senior Business Analyst Regulatory & Complianc...  14.10.2019   \n",
       "454  Senior Consultant / Manager Data Analytics (Ad...  05.11.2019   \n",
       "455        Data Sourcing, Integration & Transformation  22.10.2019   \n",
       "456                       Global Digital Media Manager  01.10.2019   \n",
       "\n",
       "                                               content           location  \n",
       "0    Job descriptionInfoCompanySwiss Life Asset Man...             Zürich  \n",
       "1    Job descriptionInfoCompany Du arbeitest bei in...             Zürich  \n",
       "2    Job descriptionInfoCompanyData Analyst Prozess...               Bern  \n",
       "3    Job descriptionInfoCompanySenior Data Scientis...      Zurich, CH-ZH  \n",
       "4    Job descriptionInfoCompanyProduct Data Analyst...    Rapperswil-Jona  \n",
       "..                                                 ...                ...  \n",
       "452  Job descriptionInfoSenior ETL Informatica and ...                NaN  \n",
       "453  Job descriptionInfoSenior Business Analyst Reg...                NaN  \n",
       "454  Job descriptionInfoCompanySenior Consultant / ...             Zürich  \n",
       "455  Job descriptionInfoCompanyData Sourcing, Integ...             Zürich  \n",
       "456  Job descriptionInfoCompanyGlobal Digital Media...         Volketswil  \n",
       "\n",
       "[457 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA SCIENTIST\n",
    "# Create the link (it is a better idea to constuct the especially link if you notice a specific pattern..)\n",
    "link_first_part = 'https://www.jobs.ch'\n",
    "link_mid_1_part = '/en/vacancies/?page='\n",
    "link_mid_2_part = '&term='\n",
    "link_mid_3_part = \"Data%20Scientist\"\n",
    "\n",
    "link = link_first_part + link_mid_1_part + link_mid_2_part + link_mid_3_part\n",
    "\n",
    "response = requests.get(link, timeout = 15)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_links = []\n",
    "def get_links(soup, job_links):\n",
    "    all_links = soup.find_all('div', class_ = 'Box-sc-7ekkso-0 Position-b2pct5-0 Position__Relative-b2pct5-1 VacancySerpItem__ShadowBox-p4qu0m-0 hthPRS')\n",
    "    for job_add in all_links:\n",
    "        job_links.append(link_first_part+job_add.find('a', {'class':'x--job-link t--job-link SearchVacancyResultsComponent__StyledVacancySerpItem-n25jij-0 dQDQbr'}).get('href'))\n",
    "\n",
    "\n",
    "\n",
    "max_pages = soup.find('div', class_ = 'Div-v2w9ke-0 Flex-sc-4aokm-0 eykbax').text.split()[2]\n",
    "\n",
    "for page in tqdm(range(1, int(max_pages)+1)[:]):\n",
    "    url = link_first_part +link_mid_1_part + str(page) + link_mid_2_part + link_mid_3_part\n",
    "    response = requests.get(url, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    get_links(soup, job_links)\n",
    "    sleep(0.6)\n",
    "\n",
    "def get_content(soup):\n",
    "    try:\n",
    "        content = soup.find('div', class_ = 'Div-v2w9ke-0 fjQgMg').get_text()\n",
    "    except:\n",
    "        content = np.nan\n",
    "    try:    \n",
    "        title = soup.find('div', {'class' : 'Div-v2w9ke-0 hPuVjT'}).find('h1').get('title')\n",
    "    except:\n",
    "        title = np.nan\n",
    "    try:\n",
    "        company = soup.find('div', class_ = 'Div-v2w9ke-0 cvwY').find('a').get('title')\n",
    "    except:\n",
    "        company = np.nan\n",
    "    try:\n",
    "        published = soup.find('span', class_ = 'Span-bhy2uh-0 Badge-ndaeev-0 krRxxu').get_text()\n",
    "    except: \n",
    "        published = np.nan\n",
    "    try:\n",
    "        location = soup.find('span', class_ = 'Span-bhy2uh-0 Text__span-sc-1vcmz87-8 YbklG Span-bhy2uh-0 Text__span-sc-1vcmz87-8 Text-sc-1vcmz87-9 gdfMMD').get_text()\n",
    "    except: \n",
    "        location = np.nan\n",
    "        \n",
    "    return content, title, company, published, location\n",
    "\n",
    "get_content(soup)\n",
    "\n",
    "cols = ['date', 'company', 'title', 'published', 'content', 'location']\n",
    "df_datascientist = pd.DataFrame(columns = cols)\n",
    "\n",
    "\n",
    "for job in tqdm(job_links[:]):\n",
    "    response = requests.get(job, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content, title, company, published, location = get_content(soup)\n",
    "    \n",
    "    df_datascientist = df_datascientist.append({\n",
    "        'date': datetime.now(), \n",
    "        'company': company, \n",
    "        'title': title, \n",
    "        'published': published, \n",
    "        'content': content,\n",
    "        'location': location\n",
    "        \n",
    "        \n",
    "    }, ignore_index = True)\n",
    "    sleep(0.6)\n",
    "\n",
    "\n",
    "df_datascientist['location'] = df_datascientist['location'].str.replace('—', '')\n",
    "df_datascientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA ANALYST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bb03987fbb405c98475c7f198893d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa9b565e6e745a986aba3373b2e5d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=474), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>content</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-16 18:43:38.288270</td>\n",
       "      <td>Swiss Life Asset Managers</td>\n",
       "      <td>Market Data Analyst</td>\n",
       "      <td>24.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanySwiss Life Asset Man...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-16 18:43:40.413052</td>\n",
       "      <td>RM IT Professional Resources AG</td>\n",
       "      <td>Data Engineer/Data Analyst</td>\n",
       "      <td>24.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyData Engineer / Data...</td>\n",
       "      <td>Bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-16 18:43:42.678876</td>\n",
       "      <td>Geberit AG</td>\n",
       "      <td>Product Data Analyst (m/w)</td>\n",
       "      <td>15.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyProduct Data Analyst...</td>\n",
       "      <td>Rapperswil-Jona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-16 18:43:44.859396</td>\n",
       "      <td>Vorwerk International &amp; Co. KmG</td>\n",
       "      <td>Senior Quality Data Analyst</td>\n",
       "      <td>14.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanySenior Quality Data ...</td>\n",
       "      <td>Wollerau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-16 18:43:47.045298</td>\n",
       "      <td>Die Schweizerische Post</td>\n",
       "      <td>Data Analyst Prozessentwicklung CRM Kundendien...</td>\n",
       "      <td>13.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyData Analyst Prozess...</td>\n",
       "      <td>Bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>2019-11-16 19:00:13.040840</td>\n",
       "      <td>Credit Suisse AG</td>\n",
       "      <td>Senior Big Data Engineer</td>\n",
       "      <td>23.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanySenior Big Data Engi...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2019-11-16 19:00:15.039116</td>\n",
       "      <td>Novartis AG</td>\n",
       "      <td>Biotransformation Scientist</td>\n",
       "      <td>10.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyBiotransformation Sc...</td>\n",
       "      <td>Basel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>2019-11-16 19:00:17.189509</td>\n",
       "      <td>F. Hoffmann-La Roche AG</td>\n",
       "      <td>Postdoctoral Fellow Drug product design method...</td>\n",
       "      <td>29.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyPostdoctoral Fellow ...</td>\n",
       "      <td>Basel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>2019-11-16 19:00:19.342773</td>\n",
       "      <td>Université de Lausanne</td>\n",
       "      <td>Bioinformatician 60%-80%</td>\n",
       "      <td>08.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyBioinformatician 60%...</td>\n",
       "      <td>Lausanne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2019-11-16 19:00:21.547435</td>\n",
       "      <td>ELCA Informatik AG</td>\n",
       "      <td>ELCA Data Science Program</td>\n",
       "      <td>24.09.2019</td>\n",
       "      <td>Job descriptionInfoCompanyELCA Data Science Pr...</td>\n",
       "      <td>Lausanne</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>474 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date                          company  \\\n",
       "0   2019-11-16 18:43:38.288270        Swiss Life Asset Managers   \n",
       "1   2019-11-16 18:43:40.413052  RM IT Professional Resources AG   \n",
       "2   2019-11-16 18:43:42.678876                       Geberit AG   \n",
       "3   2019-11-16 18:43:44.859396  Vorwerk International & Co. KmG   \n",
       "4   2019-11-16 18:43:47.045298          Die Schweizerische Post   \n",
       "..                         ...                              ...   \n",
       "469 2019-11-16 19:00:13.040840                 Credit Suisse AG   \n",
       "470 2019-11-16 19:00:15.039116                      Novartis AG   \n",
       "471 2019-11-16 19:00:17.189509          F. Hoffmann-La Roche AG   \n",
       "472 2019-11-16 19:00:19.342773           Université de Lausanne   \n",
       "473 2019-11-16 19:00:21.547435               ELCA Informatik AG   \n",
       "\n",
       "                                                 title   published  \\\n",
       "0                                  Market Data Analyst  24.10.2019   \n",
       "1                           Data Engineer/Data Analyst  24.10.2019   \n",
       "2                           Product Data Analyst (m/w)  15.11.2019   \n",
       "3                          Senior Quality Data Analyst  14.11.2019   \n",
       "4    Data Analyst Prozessentwicklung CRM Kundendien...  13.11.2019   \n",
       "..                                                 ...         ...   \n",
       "469                           Senior Big Data Engineer  23.10.2019   \n",
       "470                        Biotransformation Scientist  10.10.2019   \n",
       "471  Postdoctoral Fellow Drug product design method...  29.10.2019   \n",
       "472                           Bioinformatician 60%-80%  08.10.2019   \n",
       "473                          ELCA Data Science Program  24.09.2019   \n",
       "\n",
       "                                               content           location  \n",
       "0    Job descriptionInfoCompanySwiss Life Asset Man...             Zürich  \n",
       "1    Job descriptionInfoCompanyData Engineer / Data...               Bern  \n",
       "2    Job descriptionInfoCompanyProduct Data Analyst...    Rapperswil-Jona  \n",
       "3    Job descriptionInfoCompanySenior Quality Data ...           Wollerau  \n",
       "4    Job descriptionInfoCompanyData Analyst Prozess...               Bern  \n",
       "..                                                 ...                ...  \n",
       "469  Job descriptionInfoCompanySenior Big Data Engi...             Zürich  \n",
       "470  Job descriptionInfoCompanyBiotransformation Sc...              Basel  \n",
       "471  Job descriptionInfoCompanyPostdoctoral Fellow ...              Basel  \n",
       "472  Job descriptionInfoCompanyBioinformatician 60%...           Lausanne  \n",
       "473  Job descriptionInfoCompanyELCA Data Science Pr...           Lausanne  \n",
       "\n",
       "[474 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA ANALYST\n",
    "# Create the link (it is a better idea to constuct the especially link if you notice a specific pattern..)\n",
    "link_first_part = 'https://www.jobs.ch'\n",
    "link_mid_1_part = '/en/vacancies/?page='\n",
    "link_mid_2_part = '&term='\n",
    "link_mid_3_part = \"Data%20Analyst\"\n",
    "\n",
    "link = link_first_part + link_mid_1_part + link_mid_2_part + link_mid_3_part\n",
    "\n",
    "response = requests.get(link, timeout = 15)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_links = []\n",
    "def get_links(soup, job_links):\n",
    "    all_links = soup.find_all('div', class_ = 'Box-sc-7ekkso-0 Position-b2pct5-0 Position__Relative-b2pct5-1 VacancySerpItem__ShadowBox-p4qu0m-0 hthPRS')\n",
    "    for job_add in all_links:\n",
    "        job_links.append(link_first_part+job_add.find('a', {'class':'x--job-link t--job-link SearchVacancyResultsComponent__StyledVacancySerpItem-n25jij-0 dQDQbr'}).get('href'))\n",
    "\n",
    "\n",
    "\n",
    "max_pages = soup.find('div', class_ = 'Div-v2w9ke-0 Flex-sc-4aokm-0 eykbax').text.split()[2]\n",
    "\n",
    "for page in tqdm(range(1, int(max_pages)+1)[:]):\n",
    "    url = link_first_part +link_mid_1_part + str(page) + link_mid_2_part + link_mid_3_part\n",
    "    response = requests.get(url, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    get_links(soup, job_links)\n",
    "    sleep(0.6)\n",
    "\n",
    "def get_content(soup):\n",
    "    try:\n",
    "        content = soup.find('div', class_ = 'Div-v2w9ke-0 fjQgMg').get_text()\n",
    "    except:\n",
    "        content = np.nan\n",
    "    try:    \n",
    "        title = soup.find('div', {'class' : 'Div-v2w9ke-0 hPuVjT'}).find('h1').get('title')\n",
    "    except:\n",
    "        title = np.nan\n",
    "    try:\n",
    "        company = soup.find('div', class_ = 'Div-v2w9ke-0 cvwY').find('a').get('title')\n",
    "    except:\n",
    "        company = np.nan\n",
    "    try:\n",
    "        published = soup.find('span', class_ = 'Span-bhy2uh-0 Badge-ndaeev-0 krRxxu').get_text()\n",
    "    except: \n",
    "        published = np.nan\n",
    "    try:\n",
    "        location = soup.find('span', class_ = 'Span-bhy2uh-0 Text__span-sc-1vcmz87-8 YbklG Span-bhy2uh-0 Text__span-sc-1vcmz87-8 Text-sc-1vcmz87-9 gdfMMD').get_text()\n",
    "    except: \n",
    "        location = np.nan\n",
    "        \n",
    "    return content, title, company, published, location\n",
    "\n",
    "get_content(soup)\n",
    "\n",
    "cols = ['date', 'company', 'title', 'published', 'content', 'location']\n",
    "df_dataanalyst = pd.DataFrame(columns = cols)\n",
    "\n",
    "for job in tqdm(job_links[:]):\n",
    "    response = requests.get(job, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content, title, company, published, location = get_content(soup)\n",
    "    \n",
    "    df_dataanalyst = df_dataanalyst.append({\n",
    "        'date': datetime.now(), \n",
    "        'company': company, \n",
    "        'title': title, \n",
    "        'published': published, \n",
    "        'content': content,\n",
    "        'location': location\n",
    "        \n",
    "        \n",
    "    }, ignore_index = True)\n",
    "    sleep(0.6)\n",
    "\n",
    "\n",
    "df_dataanalyst['location'] = df_dataanalyst['location'].str.replace('—', '')\n",
    "df_dataanalyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYTHON DEVELOPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab639d5f12404151a07b8553446a5777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd1900fcfe64d11b37b9ab9aa83d602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=187), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>content</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-16 19:14:18.056235</td>\n",
       "      <td>Axpo Solutions AG</td>\n",
       "      <td>Internship Quant Developer</td>\n",
       "      <td>01.11.2019</td>\n",
       "      <td>Job descriptionInfoCompany Work closely with a...</td>\n",
       "      <td>Baden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-16 19:14:20.134483</td>\n",
       "      <td>Valora Schweiz AG</td>\n",
       "      <td>Senior Software Engineer, Fullstack 80-100%</td>\n",
       "      <td>29.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyMuttenz, ZürichSenio...</td>\n",
       "      <td>Zurich / Muttenz / Remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-16 19:14:22.251018</td>\n",
       "      <td>Camptocamp SA</td>\n",
       "      <td>Python Developer</td>\n",
       "      <td>05.11.2019</td>\n",
       "      <td>Job descriptionInfoCompany</td>\n",
       "      <td>Olten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-16 19:14:24.273791</td>\n",
       "      <td>MBA Michael Bailey Associates GmbH</td>\n",
       "      <td>DevOps Python Developer</td>\n",
       "      <td>08.11.2019</td>\n",
       "      <td>Job descriptionInfoCompany</td>\n",
       "      <td>Zurich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-16 19:14:26.551666</td>\n",
       "      <td>Labour Search GmbH</td>\n",
       "      <td>Junior Python Developer 100%</td>\n",
       "      <td>31.10.2019</td>\n",
       "      <td>Job descriptionInfoCompany</td>\n",
       "      <td>Wettingen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>2019-11-16 19:20:46.264190</td>\n",
       "      <td>Genedata AG</td>\n",
       "      <td>Scientific Consultant/Field Application Scient...</td>\n",
       "      <td>22.08.2019</td>\n",
       "      <td>Job descriptionInfoCompanyScientific Consultan...</td>\n",
       "      <td>Basel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>2019-11-16 19:20:48.290253</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>DevOps Engineer IT Security as a Service 80% b...</td>\n",
       "      <td>07.10.2019</td>\n",
       "      <td>Job descriptionInfoDevOps Engineer IT Security...</td>\n",
       "      <td>Bern, Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>2019-11-16 19:20:50.492494</td>\n",
       "      <td>Atos AG</td>\n",
       "      <td>Data Science Consultant</td>\n",
       "      <td>07.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyData Science Consult...</td>\n",
       "      <td>Basel, Basel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>2019-11-16 19:20:52.700846</td>\n",
       "      <td>QualySense AG</td>\n",
       "      <td>System Architect</td>\n",
       "      <td>29.03.2019</td>\n",
       "      <td>Job descriptionInfoCompanySystem ArchitectSYST...</td>\n",
       "      <td>Glattbrugg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>2019-11-16 19:20:54.795872</td>\n",
       "      <td>4teamwork AG</td>\n",
       "      <td>Django-Entwickler(in) (80-100%)</td>\n",
       "      <td>09.07.2019</td>\n",
       "      <td>Job descriptionInfoCompanyDjango-Entwickler(in...</td>\n",
       "      <td>Bern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date                             company  \\\n",
       "0   2019-11-16 19:14:18.056235                   Axpo Solutions AG   \n",
       "1   2019-11-16 19:14:20.134483                   Valora Schweiz AG   \n",
       "2   2019-11-16 19:14:22.251018                       Camptocamp SA   \n",
       "3   2019-11-16 19:14:24.273791  MBA Michael Bailey Associates GmbH   \n",
       "4   2019-11-16 19:14:26.551666                  Labour Search GmbH   \n",
       "..                         ...                                 ...   \n",
       "182 2019-11-16 19:20:46.264190                         Genedata AG   \n",
       "183 2019-11-16 19:20:48.290253               Swisscom (Schweiz) AG   \n",
       "184 2019-11-16 19:20:50.492494                             Atos AG   \n",
       "185 2019-11-16 19:20:52.700846                       QualySense AG   \n",
       "186 2019-11-16 19:20:54.795872                        4teamwork AG   \n",
       "\n",
       "                                                 title   published  \\\n",
       "0                           Internship Quant Developer  01.11.2019   \n",
       "1          Senior Software Engineer, Fullstack 80-100%  29.10.2019   \n",
       "2                                     Python Developer  05.11.2019   \n",
       "3                              DevOps Python Developer  08.11.2019   \n",
       "4                         Junior Python Developer 100%  31.10.2019   \n",
       "..                                                 ...         ...   \n",
       "182  Scientific Consultant/Field Application Scient...  22.08.2019   \n",
       "183  DevOps Engineer IT Security as a Service 80% b...  07.10.2019   \n",
       "184                            Data Science Consultant  07.11.2019   \n",
       "185                                   System Architect  29.03.2019   \n",
       "186                    Django-Entwickler(in) (80-100%)  09.07.2019   \n",
       "\n",
       "                                               content  \\\n",
       "0    Job descriptionInfoCompany Work closely with a...   \n",
       "1    Job descriptionInfoCompanyMuttenz, ZürichSenio...   \n",
       "2                           Job descriptionInfoCompany   \n",
       "3                           Job descriptionInfoCompany   \n",
       "4                           Job descriptionInfoCompany   \n",
       "..                                                 ...   \n",
       "182  Job descriptionInfoCompanyScientific Consultan...   \n",
       "183  Job descriptionInfoDevOps Engineer IT Security...   \n",
       "184  Job descriptionInfoCompanyData Science Consult...   \n",
       "185  Job descriptionInfoCompanySystem ArchitectSYST...   \n",
       "186  Job descriptionInfoCompanyDjango-Entwickler(in...   \n",
       "\n",
       "                        location  \n",
       "0                          Baden  \n",
       "1      Zurich / Muttenz / Remote  \n",
       "2                          Olten  \n",
       "3                         Zurich  \n",
       "4                      Wettingen  \n",
       "..                           ...  \n",
       "182                        Basel  \n",
       "183                 Bern, Zürich  \n",
       "184                 Basel, Basel  \n",
       "185                   Glattbrugg  \n",
       "186                         Bern  \n",
       "\n",
       "[187 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PYTHON DEVELOPER\n",
    "# Create the link (it is a better idea to constuct the especially link if you notice a specific pattern..)\n",
    "link_first_part = 'https://www.jobs.ch'\n",
    "link_mid_1_part = '/en/vacancies/?page='\n",
    "link_mid_2_part = '&term='\n",
    "link_mid_3_part = \"Python%20Developer\"\n",
    "\n",
    "link = link_first_part + link_mid_1_part + link_mid_2_part + link_mid_3_part\n",
    "\n",
    "response = requests.get(link, timeout = 15)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_links = []\n",
    "def get_links(soup, job_links):\n",
    "    all_links = soup.find_all('div', class_ = 'Box-sc-7ekkso-0 Position-b2pct5-0 Position__Relative-b2pct5-1 VacancySerpItem__ShadowBox-p4qu0m-0 hthPRS')\n",
    "    for job_add in all_links:\n",
    "        job_links.append(link_first_part+job_add.find('a', {'class':'x--job-link t--job-link SearchVacancyResultsComponent__StyledVacancySerpItem-n25jij-0 dQDQbr'}).get('href'))\n",
    "\n",
    "\n",
    "\n",
    "max_pages = soup.find('div', class_ = 'Div-v2w9ke-0 Flex-sc-4aokm-0 eykbax').text.split()[2]\n",
    "\n",
    "for page in tqdm(range(1, int(max_pages)+1)[:]):\n",
    "    url = link_first_part +link_mid_1_part + str(page) + link_mid_2_part + link_mid_3_part\n",
    "    response = requests.get(url, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    get_links(soup, job_links)\n",
    "    sleep(0.6)\n",
    "\n",
    "def get_content(soup):\n",
    "    try:\n",
    "        content = soup.find('div', class_ = 'Div-v2w9ke-0 fjQgMg').get_text()\n",
    "    except:\n",
    "        content = np.nan\n",
    "    try:    \n",
    "        title = soup.find('div', {'class' : 'Div-v2w9ke-0 hPuVjT'}).find('h1').get('title')\n",
    "    except:\n",
    "        title = np.nan\n",
    "    try:\n",
    "        company = soup.find('div', class_ = 'Div-v2w9ke-0 cvwY').find('a').get('title')\n",
    "    except:\n",
    "        company = np.nan\n",
    "    try:\n",
    "        published = soup.find('span', class_ = 'Span-bhy2uh-0 Badge-ndaeev-0 krRxxu').get_text()\n",
    "    except: \n",
    "        published = np.nan\n",
    "    try:\n",
    "        location = soup.find('span', class_ = 'Span-bhy2uh-0 Text__span-sc-1vcmz87-8 YbklG Span-bhy2uh-0 Text__span-sc-1vcmz87-8 Text-sc-1vcmz87-9 gdfMMD').get_text()\n",
    "    except: \n",
    "        location = np.nan\n",
    "        \n",
    "    return content, title, company, published, location\n",
    "\n",
    "get_content(soup)\n",
    "\n",
    "cols = ['date', 'company', 'title', 'published', 'content', 'location']\n",
    "df_pythondeveloper = pd.DataFrame(columns = cols)\n",
    "\n",
    "\n",
    "for job in tqdm(job_links[:]):\n",
    "    response = requests.get(job, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content, title, company, published, location = get_content(soup)\n",
    "    \n",
    "    df_pythondeveloper = df_pythondeveloper.append({\n",
    "        'date': datetime.now(), \n",
    "        'company': company, \n",
    "        'title': title, \n",
    "        'published': published, \n",
    "        'content': content,\n",
    "        'location': location\n",
    "        \n",
    "        \n",
    "    }, ignore_index = True)\n",
    "    sleep(0.6)\n",
    "\n",
    "\n",
    "df_pythondeveloper['location'] = df_pythondeveloper['location'].str.replace('—', '')\n",
    "df_pythondeveloper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA ENGINEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7738006ec5844e0eb7cb8d99cb7152c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=49), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd175d9c407f484ab5c323277f759330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=981), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>content</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-16 19:35:49.321755</td>\n",
       "      <td>Leica Geosystems AG</td>\n",
       "      <td>Machine Learning Engineer (f/m)</td>\n",
       "      <td>09.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyMachine Learning Eng...</td>\n",
       "      <td>Heerbrugg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-16 19:35:51.546587</td>\n",
       "      <td>Eniwa AG</td>\n",
       "      <td>Enterprise Data Engineer 80-100% (m/w/d)</td>\n",
       "      <td>01.11.2019</td>\n",
       "      <td>Job descriptionInfoCompany Ausarbeiten der unt...</td>\n",
       "      <td>Buchs AG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-16 19:35:53.674975</td>\n",
       "      <td>Experis Schweiz Zürich</td>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>04.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanySenior Data Engineer...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-16 19:35:55.837169</td>\n",
       "      <td>The Stamford Group AG</td>\n",
       "      <td>DevOps Engineer</td>\n",
       "      <td>13.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyDevOps EngineerJob D...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-16 19:35:58.206135</td>\n",
       "      <td>Stamford Consultants AG</td>\n",
       "      <td>Cloud Site Reliability Engineer</td>\n",
       "      <td>15.11.2019</td>\n",
       "      <td>Job descriptionInfoCompany</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>2019-11-16 20:10:39.629413</td>\n",
       "      <td>TE Connectivity Solutions GmbH</td>\n",
       "      <td>SUPPLIER DEVELOPMENT ANALYST IV</td>\n",
       "      <td>08.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanySUPPLIER DEVELOPMENT...</td>\n",
       "      <td>Chochin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>2019-11-16 20:10:41.559033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Senior Account Executive - Manufacturing</td>\n",
       "      <td>05.06.2019</td>\n",
       "      <td>Job descriptionInfoSenior Account Executive - ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>2019-11-16 20:10:43.784260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CONSULTANT FÜR SYSTEMARCHITEKTUR GA / ICT</td>\n",
       "      <td>09.07.2019</td>\n",
       "      <td>Job descriptionInfoCONSULTANT FÜR SYSTEMARCHIT...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>2019-11-16 20:10:46.122390</td>\n",
       "      <td>ti&amp;m AG</td>\n",
       "      <td>(Senior) Consultant Digital Transformation</td>\n",
       "      <td>24.01.2019</td>\n",
       "      <td>Job descriptionInfoCompany(Senior) Consultant ...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>2019-11-16 20:10:48.377828</td>\n",
       "      <td>ELCA Informatik AG</td>\n",
       "      <td>Building the next electronic identity (E-ID) f...</td>\n",
       "      <td>18.09.2019</td>\n",
       "      <td>Job descriptionInfoCompanyBuilding the next el...</td>\n",
       "      <td>Lausanne</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>981 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date                         company  \\\n",
       "0   2019-11-16 19:35:49.321755             Leica Geosystems AG   \n",
       "1   2019-11-16 19:35:51.546587                        Eniwa AG   \n",
       "2   2019-11-16 19:35:53.674975          Experis Schweiz Zürich   \n",
       "3   2019-11-16 19:35:55.837169           The Stamford Group AG   \n",
       "4   2019-11-16 19:35:58.206135         Stamford Consultants AG   \n",
       "..                         ...                             ...   \n",
       "976 2019-11-16 20:10:39.629413  TE Connectivity Solutions GmbH   \n",
       "977 2019-11-16 20:10:41.559033                             NaN   \n",
       "978 2019-11-16 20:10:43.784260                             NaN   \n",
       "979 2019-11-16 20:10:46.122390                         ti&m AG   \n",
       "980 2019-11-16 20:10:48.377828              ELCA Informatik AG   \n",
       "\n",
       "                                                 title   published  \\\n",
       "0                      Machine Learning Engineer (f/m)  09.11.2019   \n",
       "1             Enterprise Data Engineer 80-100% (m/w/d)  01.11.2019   \n",
       "2                                 Senior Data Engineer  04.11.2019   \n",
       "3                                      DevOps Engineer  13.11.2019   \n",
       "4                      Cloud Site Reliability Engineer  15.11.2019   \n",
       "..                                                 ...         ...   \n",
       "976                    SUPPLIER DEVELOPMENT ANALYST IV  08.10.2019   \n",
       "977           Senior Account Executive - Manufacturing  05.06.2019   \n",
       "978          CONSULTANT FÜR SYSTEMARCHITEKTUR GA / ICT  09.07.2019   \n",
       "979         (Senior) Consultant Digital Transformation  24.01.2019   \n",
       "980  Building the next electronic identity (E-ID) f...  18.09.2019   \n",
       "\n",
       "                                               content     location  \n",
       "0    Job descriptionInfoCompanyMachine Learning Eng...    Heerbrugg  \n",
       "1    Job descriptionInfoCompany Ausarbeiten der unt...     Buchs AG  \n",
       "2    Job descriptionInfoCompanySenior Data Engineer...       Zürich  \n",
       "3    Job descriptionInfoCompanyDevOps EngineerJob D...       Zürich  \n",
       "4                           Job descriptionInfoCompany       Zürich  \n",
       "..                                                 ...          ...  \n",
       "976  Job descriptionInfoCompanySUPPLIER DEVELOPMENT...      Chochin  \n",
       "977  Job descriptionInfoSenior Account Executive - ...          NaN  \n",
       "978  Job descriptionInfoCONSULTANT FÜR SYSTEMARCHIT...          NaN  \n",
       "979  Job descriptionInfoCompany(Senior) Consultant ...       Zürich  \n",
       "980  Job descriptionInfoCompanyBuilding the next el...     Lausanne  \n",
       "\n",
       "[981 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA ENGINEER\n",
    "# Create the link (it is a better idea to constuct the especially link if you notice a specific pattern..)\n",
    "link_first_part = 'https://www.jobs.ch'\n",
    "link_mid_1_part = '/en/vacancies/?page='\n",
    "link_mid_2_part = '&term='\n",
    "link_mid_3_part = \"Data%20Engineer\"\n",
    "\n",
    "link = link_first_part + link_mid_1_part + link_mid_2_part + link_mid_3_part\n",
    "\n",
    "response = requests.get(link, timeout = 15)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_links = []\n",
    "def get_links(soup, job_links):\n",
    "    all_links = soup.find_all('div', class_ = 'Box-sc-7ekkso-0 Position-b2pct5-0 Position__Relative-b2pct5-1 VacancySerpItem__ShadowBox-p4qu0m-0 hthPRS')\n",
    "    for job_add in all_links:\n",
    "        job_links.append(link_first_part+job_add.find('a', {'class':'x--job-link t--job-link SearchVacancyResultsComponent__StyledVacancySerpItem-n25jij-0 dQDQbr'}).get('href'))\n",
    "\n",
    "\n",
    "\n",
    "max_pages = soup.find('div', class_ = 'Div-v2w9ke-0 Flex-sc-4aokm-0 eykbax').text.split()[2]\n",
    "\n",
    "for page in tqdm(range(1, int(max_pages)+1)[:]):\n",
    "    url = link_first_part +link_mid_1_part + str(page) + link_mid_2_part + link_mid_3_part\n",
    "    response = requests.get(url, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    get_links(soup, job_links)\n",
    "    sleep(0.6)\n",
    "\n",
    "def get_content(soup):\n",
    "    try:\n",
    "        content = soup.find('div', class_ = 'Div-v2w9ke-0 fjQgMg').get_text()\n",
    "    except:\n",
    "        content = np.nan\n",
    "    try:    \n",
    "        title = soup.find('div', {'class' : 'Div-v2w9ke-0 hPuVjT'}).find('h1').get('title')\n",
    "    except:\n",
    "        title = np.nan\n",
    "    try:\n",
    "        company = soup.find('div', class_ = 'Div-v2w9ke-0 cvwY').find('a').get('title')\n",
    "    except:\n",
    "        company = np.nan\n",
    "    try:\n",
    "        published = soup.find('span', class_ = 'Span-bhy2uh-0 Badge-ndaeev-0 krRxxu').get_text()\n",
    "    except: \n",
    "        published = np.nan\n",
    "    try:\n",
    "        location = soup.find('span', class_ = 'Span-bhy2uh-0 Text__span-sc-1vcmz87-8 YbklG Span-bhy2uh-0 Text__span-sc-1vcmz87-8 Text-sc-1vcmz87-9 gdfMMD').get_text()\n",
    "    except: \n",
    "        location = np.nan\n",
    "        \n",
    "    return content, title, company, published, location\n",
    "\n",
    "get_content(soup)\n",
    "\n",
    "cols = ['date', 'company', 'title', 'published', 'content', 'location']\n",
    "df_dataengineer = pd.DataFrame(columns = cols)\n",
    "\n",
    "\n",
    "for job in tqdm(job_links[:]):\n",
    "    response = requests.get(job, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content, title, company, published, location = get_content(soup)\n",
    "    \n",
    "    df_dataengineer = df_dataengineer.append({\n",
    "        'date': datetime.now(), \n",
    "        'company': company, \n",
    "        'title': title, \n",
    "        'published': published, \n",
    "        'content': content,\n",
    "        'location': location\n",
    "        \n",
    "        \n",
    "    }, ignore_index = True)\n",
    "    sleep(0.6)\n",
    "\n",
    "\n",
    "df_dataengineer['location'] = df_dataengineer['location'].str.replace('—', '')\n",
    "df_dataengineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA MANAGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803c8840d7e740628285891638e648f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=66), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8e640ba19641e8ac896d5d7955d761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1335), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>content</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-16 20:33:54.926564</td>\n",
       "      <td>Luzerner Kantonsspital</td>\n",
       "      <td>Datenmanager/in 80%</td>\n",
       "      <td>28.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyDatenmanager/in 80%I...</td>\n",
       "      <td>Luzern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-16 20:33:57.016967</td>\n",
       "      <td>LGT</td>\n",
       "      <td>Client Data Officer (100%)</td>\n",
       "      <td>01.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyClient Data Officer ...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-16 20:33:59.834496</td>\n",
       "      <td>Sonova</td>\n",
       "      <td>Data Protection Manager</td>\n",
       "      <td>18.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyData Protection Mana...</td>\n",
       "      <td>Stäfa und Zug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-16 20:34:01.849725</td>\n",
       "      <td>maxon motor ag</td>\n",
       "      <td>Service Responsible Datacenter / Oracle 90-100...</td>\n",
       "      <td>06.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyFür unsere Abteilung...</td>\n",
       "      <td>Sachseln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-16 20:34:04.003303</td>\n",
       "      <td>Geberit AG</td>\n",
       "      <td>Master Data Manager (m/w)</td>\n",
       "      <td>15.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyMaster Data Manager ...</td>\n",
       "      <td>Rapperswil/Jona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>2019-11-16 21:21:41.128190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Technical Business Analyst - Swiss Social Insu...</td>\n",
       "      <td>24.09.2019</td>\n",
       "      <td>Job descriptionInfoTechnical Business Analyst ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>2019-11-16 21:21:43.432278</td>\n",
       "      <td>Jones Lang LaSalle AG</td>\n",
       "      <td>Sales &amp; Solutions Director</td>\n",
       "      <td>17.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanySales &amp; Solutions Di...</td>\n",
       "      <td>Frankfurt, DEU, Mehr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>2019-11-16 21:21:45.549402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Associate Solutions Engineer - Bachelor/Master...</td>\n",
       "      <td>01.10.2019</td>\n",
       "      <td>Job descriptionInfoAssociate Solutions Enginee...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>2019-11-16 21:21:47.451995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fachspezialist/-in Datenmanagement / Pflanzenö...</td>\n",
       "      <td>30.10.2019</td>\n",
       "      <td>Job descriptionInfoFachspezialist/-in Datenman...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>2019-11-16 21:21:49.425520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADMINISTRATIVE OFFICER</td>\n",
       "      <td>01.11.2019</td>\n",
       "      <td>Job descriptionInfoADMINISTRATIVE OFFICER    O...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1335 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date                 company  \\\n",
       "0    2019-11-16 20:33:54.926564  Luzerner Kantonsspital   \n",
       "1    2019-11-16 20:33:57.016967                     LGT   \n",
       "2    2019-11-16 20:33:59.834496                  Sonova   \n",
       "3    2019-11-16 20:34:01.849725          maxon motor ag   \n",
       "4    2019-11-16 20:34:04.003303              Geberit AG   \n",
       "...                         ...                     ...   \n",
       "1330 2019-11-16 21:21:41.128190                     NaN   \n",
       "1331 2019-11-16 21:21:43.432278   Jones Lang LaSalle AG   \n",
       "1332 2019-11-16 21:21:45.549402                     NaN   \n",
       "1333 2019-11-16 21:21:47.451995                     NaN   \n",
       "1334 2019-11-16 21:21:49.425520                     NaN   \n",
       "\n",
       "                                                  title   published  \\\n",
       "0                                   Datenmanager/in 80%  28.10.2019   \n",
       "1                            Client Data Officer (100%)  01.11.2019   \n",
       "2                               Data Protection Manager  18.10.2019   \n",
       "3     Service Responsible Datacenter / Oracle 90-100...  06.11.2019   \n",
       "4                             Master Data Manager (m/w)  15.11.2019   \n",
       "...                                                 ...         ...   \n",
       "1330  Technical Business Analyst - Swiss Social Insu...  24.09.2019   \n",
       "1331                         Sales & Solutions Director  17.10.2019   \n",
       "1332  Associate Solutions Engineer - Bachelor/Master...  01.10.2019   \n",
       "1333  Fachspezialist/-in Datenmanagement / Pflanzenö...  30.10.2019   \n",
       "1334                             ADMINISTRATIVE OFFICER  01.11.2019   \n",
       "\n",
       "                                                content  \\\n",
       "0     Job descriptionInfoCompanyDatenmanager/in 80%I...   \n",
       "1     Job descriptionInfoCompanyClient Data Officer ...   \n",
       "2     Job descriptionInfoCompanyData Protection Mana...   \n",
       "3     Job descriptionInfoCompanyFür unsere Abteilung...   \n",
       "4     Job descriptionInfoCompanyMaster Data Manager ...   \n",
       "...                                                 ...   \n",
       "1330  Job descriptionInfoTechnical Business Analyst ...   \n",
       "1331  Job descriptionInfoCompanySales & Solutions Di...   \n",
       "1332  Job descriptionInfoAssociate Solutions Enginee...   \n",
       "1333  Job descriptionInfoFachspezialist/-in Datenman...   \n",
       "1334  Job descriptionInfoADMINISTRATIVE OFFICER    O...   \n",
       "\n",
       "                       location  \n",
       "0                        Luzern  \n",
       "1                        Zürich  \n",
       "2                 Stäfa und Zug  \n",
       "3                      Sachseln  \n",
       "4               Rapperswil/Jona  \n",
       "...                         ...  \n",
       "1330                        NaN  \n",
       "1331    Frankfurt, DEU, Mehr...  \n",
       "1332                        NaN  \n",
       "1333                        NaN  \n",
       "1334                        NaN  \n",
       "\n",
       "[1335 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA MANAGER\n",
    "# Create the link (it is a better idea to constuct the especially link if you notice a specific pattern..)\n",
    "link_first_part = 'https://www.jobs.ch'\n",
    "link_mid_1_part = '/en/vacancies/?page='\n",
    "link_mid_2_part = '&term='\n",
    "link_mid_3_part = \"Data%20Manager\"\n",
    "\n",
    "link = link_first_part + link_mid_1_part + link_mid_2_part + link_mid_3_part\n",
    "\n",
    "response = requests.get(link, timeout = 15)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_links = []\n",
    "def get_links(soup, job_links):\n",
    "    all_links = soup.find_all('div', class_ = 'Box-sc-7ekkso-0 Position-b2pct5-0 Position__Relative-b2pct5-1 VacancySerpItem__ShadowBox-p4qu0m-0 hthPRS')\n",
    "    for job_add in all_links:\n",
    "        job_links.append(link_first_part+job_add.find('a', {'class':'x--job-link t--job-link SearchVacancyResultsComponent__StyledVacancySerpItem-n25jij-0 dQDQbr'}).get('href'))\n",
    "\n",
    "\n",
    "\n",
    "max_pages = soup.find('div', class_ = 'Div-v2w9ke-0 Flex-sc-4aokm-0 eykbax').text.split()[2]\n",
    "\n",
    "for page in tqdm(range(1, int(max_pages)+1)[:]):\n",
    "    url = link_first_part +link_mid_1_part + str(page) + link_mid_2_part + link_mid_3_part\n",
    "    response = requests.get(url, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    get_links(soup, job_links)\n",
    "    sleep(0.6)\n",
    "\n",
    "def get_content(soup):\n",
    "    try:\n",
    "        content = soup.find('div', class_ = 'Div-v2w9ke-0 fjQgMg').get_text()\n",
    "    except:\n",
    "        content = np.nan\n",
    "    try:    \n",
    "        title = soup.find('div', {'class' : 'Div-v2w9ke-0 hPuVjT'}).find('h1').get('title')\n",
    "    except:\n",
    "        title = np.nan\n",
    "    try:\n",
    "        company = soup.find('div', class_ = 'Div-v2w9ke-0 cvwY').find('a').get('title')\n",
    "    except:\n",
    "        company = np.nan\n",
    "    try:\n",
    "        published = soup.find('span', class_ = 'Span-bhy2uh-0 Badge-ndaeev-0 krRxxu').get_text()\n",
    "    except: \n",
    "        published = np.nan\n",
    "    try:\n",
    "        location = soup.find('span', class_ = 'Span-bhy2uh-0 Text__span-sc-1vcmz87-8 YbklG Span-bhy2uh-0 Text__span-sc-1vcmz87-8 Text-sc-1vcmz87-9 gdfMMD').get_text()\n",
    "    except: \n",
    "        location = np.nan\n",
    "        \n",
    "    return content, title, company, published, location\n",
    "\n",
    "get_content(soup)\n",
    "\n",
    "cols = ['date', 'company', 'title', 'published', 'content', 'location']\n",
    "df_datamanager = pd.DataFrame(columns = cols)\n",
    "\n",
    "\n",
    "for job in tqdm(job_links[:]):\n",
    "    response = requests.get(job, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content, title, company, published, location = get_content(soup)\n",
    "    \n",
    "    df_datamanager = df_datamanager.append({\n",
    "        'date': datetime.now(), \n",
    "        'company': company, \n",
    "        'title': title, \n",
    "        'published': published, \n",
    "        'content': content,\n",
    "        'location': location\n",
    "        \n",
    "        \n",
    "    }, ignore_index = True)\n",
    "    sleep(0.6)\n",
    "\n",
    "\n",
    "df_datamanager['location'] = df_datamanager['location'].str.replace('—', '')\n",
    "df_datamanager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA ARCHITECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af001962ec994cf2936954a1df4a4616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6cb44b1f9b4aeab44cd39958527dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=230), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>content</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-16 21:24:04.572754</td>\n",
       "      <td>Banian AG</td>\n",
       "      <td>Data Engineer / Solution Architect</td>\n",
       "      <td>28.10.2019</td>\n",
       "      <td>Job descriptionInfoCompany</td>\n",
       "      <td>Deutschschweiz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-16 21:24:06.584656</td>\n",
       "      <td>Syngenta</td>\n",
       "      <td>Senior Solution Architect E-commerce</td>\n",
       "      <td>07.11.2019</td>\n",
       "      <td>Job descriptionInfoCompany</td>\n",
       "      <td>Basel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-16 21:24:08.543106</td>\n",
       "      <td>RM IT Professional Resources AG</td>\n",
       "      <td>Data Warehouse Developer - Oracle PL/SQL</td>\n",
       "      <td>15.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyData Warehouse Devel...</td>\n",
       "      <td>Bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-16 21:24:10.651867</td>\n",
       "      <td>Credit Suisse AG</td>\n",
       "      <td>Agile DWH Analyst 80 - 100%</td>\n",
       "      <td>21.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyAgile DWH Analyst 80...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-16 21:24:12.632350</td>\n",
       "      <td>ROSEN Swiss AG</td>\n",
       "      <td>Microsoft Data Warehouse/BI-Entwickler (m/w)</td>\n",
       "      <td>08.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyJobbeschreibung Mit ...</td>\n",
       "      <td>Stans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2019-11-16 21:32:13.498389</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>Performance Testing Engineer 80% bis 100%</td>\n",
       "      <td>30.09.2019</td>\n",
       "      <td>Job descriptionInfoPerformance Testing Enginee...</td>\n",
       "      <td>Liebefeld</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2019-11-16 21:32:15.697258</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>DevOps Engineer Container Platform 80% bis 100%</td>\n",
       "      <td>04.09.2019</td>\n",
       "      <td>Job descriptionInfoDevOps Engineer Container P...</td>\n",
       "      <td>Zürich oder Bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2019-11-16 21:32:18.348728</td>\n",
       "      <td>Vicara Infotech Group AG</td>\n",
       "      <td>DWH EXPERT</td>\n",
       "      <td>04.12.2017</td>\n",
       "      <td>Job descriptionInfoCompanyDWH EXPERT  We are l...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>2019-11-16 21:32:20.360377</td>\n",
       "      <td>Softcom Technologies SA</td>\n",
       "      <td>Senior Entwickler - Architekt (m/w) 80-100%</td>\n",
       "      <td>30.08.2019</td>\n",
       "      <td>Job descriptionInfoSenior Entwickler - Archite...</td>\n",
       "      <td>Bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>2019-11-16 21:32:22.400782</td>\n",
       "      <td>Valiant Bank AG</td>\n",
       "      <td>Senior DWH Spezialist m/w 80 -100 %</td>\n",
       "      <td>07.03.2019</td>\n",
       "      <td>Job descriptionInfoSenior DWH Spezialist m/w 8...</td>\n",
       "      <td>Bern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date                          company  \\\n",
       "0   2019-11-16 21:24:04.572754                        Banian AG   \n",
       "1   2019-11-16 21:24:06.584656                         Syngenta   \n",
       "2   2019-11-16 21:24:08.543106  RM IT Professional Resources AG   \n",
       "3   2019-11-16 21:24:10.651867                 Credit Suisse AG   \n",
       "4   2019-11-16 21:24:12.632350                   ROSEN Swiss AG   \n",
       "..                         ...                              ...   \n",
       "225 2019-11-16 21:32:13.498389            Swisscom (Schweiz) AG   \n",
       "226 2019-11-16 21:32:15.697258            Swisscom (Schweiz) AG   \n",
       "227 2019-11-16 21:32:18.348728         Vicara Infotech Group AG   \n",
       "228 2019-11-16 21:32:20.360377          Softcom Technologies SA   \n",
       "229 2019-11-16 21:32:22.400782                  Valiant Bank AG   \n",
       "\n",
       "                                               title   published  \\\n",
       "0                 Data Engineer / Solution Architect  28.10.2019   \n",
       "1               Senior Solution Architect E-commerce  07.11.2019   \n",
       "2           Data Warehouse Developer - Oracle PL/SQL  15.11.2019   \n",
       "3                        Agile DWH Analyst 80 - 100%  21.10.2019   \n",
       "4       Microsoft Data Warehouse/BI-Entwickler (m/w)  08.11.2019   \n",
       "..                                               ...         ...   \n",
       "225        Performance Testing Engineer 80% bis 100%  30.09.2019   \n",
       "226  DevOps Engineer Container Platform 80% bis 100%  04.09.2019   \n",
       "227                                       DWH EXPERT  04.12.2017   \n",
       "228      Senior Entwickler - Architekt (m/w) 80-100%  30.08.2019   \n",
       "229              Senior DWH Spezialist m/w 80 -100 %  07.03.2019   \n",
       "\n",
       "                                               content            location  \n",
       "0                           Job descriptionInfoCompany      Deutschschweiz  \n",
       "1                           Job descriptionInfoCompany               Basel  \n",
       "2    Job descriptionInfoCompanyData Warehouse Devel...                Bern  \n",
       "3    Job descriptionInfoCompanyAgile DWH Analyst 80...              Zürich  \n",
       "4    Job descriptionInfoCompanyJobbeschreibung Mit ...               Stans  \n",
       "..                                                 ...                 ...  \n",
       "225  Job descriptionInfoPerformance Testing Enginee...           Liebefeld  \n",
       "226  Job descriptionInfoDevOps Engineer Container P...    Zürich oder Bern  \n",
       "227  Job descriptionInfoCompanyDWH EXPERT  We are l...              Zürich  \n",
       "228  Job descriptionInfoSenior Entwickler - Archite...                Bern  \n",
       "229  Job descriptionInfoSenior DWH Spezialist m/w 8...                Bern  \n",
       "\n",
       "[230 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA ARCHITECT\n",
    "# Create the link (it is a better idea to constuct the especially link if you notice a specific pattern..)\n",
    "link_first_part = 'https://www.jobs.ch'\n",
    "link_mid_1_part = '/en/vacancies/?page='\n",
    "link_mid_2_part = '&term='\n",
    "link_mid_3_part = \"Data%20Architect\"\n",
    "\n",
    "link = link_first_part + link_mid_1_part + link_mid_2_part + link_mid_3_part\n",
    "\n",
    "response = requests.get(link, timeout = 15)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_links = []\n",
    "def get_links(soup, job_links):\n",
    "    all_links = soup.find_all('div', class_ = 'Box-sc-7ekkso-0 Position-b2pct5-0 Position__Relative-b2pct5-1 VacancySerpItem__ShadowBox-p4qu0m-0 hthPRS')\n",
    "    for job_add in all_links:\n",
    "        job_links.append(link_first_part+job_add.find('a', {'class':'x--job-link t--job-link SearchVacancyResultsComponent__StyledVacancySerpItem-n25jij-0 dQDQbr'}).get('href'))\n",
    "\n",
    "\n",
    "\n",
    "max_pages = soup.find('div', class_ = 'Div-v2w9ke-0 Flex-sc-4aokm-0 eykbax').text.split()[2]\n",
    "\n",
    "for page in tqdm(range(1, int(max_pages)+1)[:]):\n",
    "    url = link_first_part +link_mid_1_part + str(page) + link_mid_2_part + link_mid_3_part\n",
    "    response = requests.get(url, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    get_links(soup, job_links)\n",
    "    sleep(0.6)\n",
    "\n",
    "def get_content(soup):\n",
    "    try:\n",
    "        content = soup.find('div', class_ = 'Div-v2w9ke-0 fjQgMg').get_text()\n",
    "    except:\n",
    "        content = np.nan\n",
    "    try:    \n",
    "        title = soup.find('div', {'class' : 'Div-v2w9ke-0 hPuVjT'}).find('h1').get('title')\n",
    "    except:\n",
    "        title = np.nan\n",
    "    try:\n",
    "        company = soup.find('div', class_ = 'Div-v2w9ke-0 cvwY').find('a').get('title')\n",
    "    except:\n",
    "        company = np.nan\n",
    "    try:\n",
    "        published = soup.find('span', class_ = 'Span-bhy2uh-0 Badge-ndaeev-0 krRxxu').get_text()\n",
    "    except: \n",
    "        published = np.nan\n",
    "    try:\n",
    "        location = soup.find('span', class_ = 'Span-bhy2uh-0 Text__span-sc-1vcmz87-8 YbklG Span-bhy2uh-0 Text__span-sc-1vcmz87-8 Text-sc-1vcmz87-9 gdfMMD').get_text()\n",
    "    except: \n",
    "        location = np.nan\n",
    "        \n",
    "    return content, title, company, published, location\n",
    "\n",
    "get_content(soup)\n",
    "\n",
    "cols = ['date', 'company', 'title', 'published', 'content', 'location']\n",
    "df_dataarchitect = pd.DataFrame(columns = cols)\n",
    "\n",
    "\n",
    "for job in tqdm(job_links[:]):\n",
    "    response = requests.get(job, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content, title, company, published, location = get_content(soup)\n",
    "    \n",
    "    df_dataarchitect = df_dataarchitect.append({\n",
    "        'date': datetime.now(), \n",
    "        'company': company, \n",
    "        'title': title, \n",
    "        'published': published, \n",
    "        'content': content,\n",
    "        'location': location\n",
    "        \n",
    "        \n",
    "    }, ignore_index = True)\n",
    "    sleep(0.6)\n",
    "\n",
    "\n",
    "df_dataarchitect['location'] = df_dataarchitect['location'].str.replace('—', '')\n",
    "df_dataarchitect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIG DATA ANALYST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37570e6f29174dc5a05a94828dfc779b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25af70041690487e99ccf02fee8095e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=210), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>content</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-16 22:20:16.236028</td>\n",
       "      <td>Swiss Life Asset Managers</td>\n",
       "      <td>Market Data Analyst</td>\n",
       "      <td>24.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanySwiss Life Asset Man...</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-16 22:20:18.263458</td>\n",
       "      <td>RM IT Professional Resources AG</td>\n",
       "      <td>Data Engineer/Data Analyst</td>\n",
       "      <td>24.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyData Engineer / Data...</td>\n",
       "      <td>Bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-16 22:20:20.423064</td>\n",
       "      <td>Die Schweizerische Post</td>\n",
       "      <td>Data Analyst Prozessentwicklung CRM Kundendien...</td>\n",
       "      <td>13.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyData Analyst Prozess...</td>\n",
       "      <td>Bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-16 22:20:23.324456</td>\n",
       "      <td>Geberit AG</td>\n",
       "      <td>Product Data Analyst (m/w)</td>\n",
       "      <td>15.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyProduct Data Analyst...</td>\n",
       "      <td>Rapperswil-Jona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-16 22:20:25.482604</td>\n",
       "      <td>Vorwerk International &amp; Co. KmG</td>\n",
       "      <td>Senior Quality Data Analyst</td>\n",
       "      <td>14.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanySenior Quality Data ...</td>\n",
       "      <td>Wollerau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>2019-11-16 22:27:40.209296</td>\n",
       "      <td>La Prairie Group AG</td>\n",
       "      <td>Global Digital Media Manager</td>\n",
       "      <td>01.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyGlobal Digital Media...</td>\n",
       "      <td>Volketswil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>2019-11-16 22:27:42.307424</td>\n",
       "      <td>F. Hoffmann-La Roche AG</td>\n",
       "      <td>Postdoctoral Fellow Drug product design method...</td>\n",
       "      <td>29.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyPostdoctoral Fellow ...</td>\n",
       "      <td>Basel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2019-11-16 22:27:44.434497</td>\n",
       "      <td>Novartis AG</td>\n",
       "      <td>DSAI Advanced Visual Analytics Scientist</td>\n",
       "      <td>28.07.2019</td>\n",
       "      <td>Job descriptionInfoCompanyDSAI Advanced Visual...</td>\n",
       "      <td>Basel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>2019-11-16 22:27:46.473890</td>\n",
       "      <td>Université de Lausanne</td>\n",
       "      <td>Bioinformatician 60%-80%</td>\n",
       "      <td>08.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanyBioinformatician 60%...</td>\n",
       "      <td>Lausanne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>2019-11-16 22:27:48.591026</td>\n",
       "      <td>ELCA Informatik AG</td>\n",
       "      <td>ELCA Data Science Program</td>\n",
       "      <td>24.09.2019</td>\n",
       "      <td>Job descriptionInfoCompanyELCA Data Science Pr...</td>\n",
       "      <td>Lausanne</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date                          company  \\\n",
       "0   2019-11-16 22:20:16.236028        Swiss Life Asset Managers   \n",
       "1   2019-11-16 22:20:18.263458  RM IT Professional Resources AG   \n",
       "2   2019-11-16 22:20:20.423064          Die Schweizerische Post   \n",
       "3   2019-11-16 22:20:23.324456                       Geberit AG   \n",
       "4   2019-11-16 22:20:25.482604  Vorwerk International & Co. KmG   \n",
       "..                         ...                              ...   \n",
       "205 2019-11-16 22:27:40.209296              La Prairie Group AG   \n",
       "206 2019-11-16 22:27:42.307424          F. Hoffmann-La Roche AG   \n",
       "207 2019-11-16 22:27:44.434497                      Novartis AG   \n",
       "208 2019-11-16 22:27:46.473890           Université de Lausanne   \n",
       "209 2019-11-16 22:27:48.591026               ELCA Informatik AG   \n",
       "\n",
       "                                                 title   published  \\\n",
       "0                                  Market Data Analyst  24.10.2019   \n",
       "1                           Data Engineer/Data Analyst  24.10.2019   \n",
       "2    Data Analyst Prozessentwicklung CRM Kundendien...  13.11.2019   \n",
       "3                           Product Data Analyst (m/w)  15.11.2019   \n",
       "4                          Senior Quality Data Analyst  14.11.2019   \n",
       "..                                                 ...         ...   \n",
       "205                       Global Digital Media Manager  01.10.2019   \n",
       "206  Postdoctoral Fellow Drug product design method...  29.10.2019   \n",
       "207           DSAI Advanced Visual Analytics Scientist  28.07.2019   \n",
       "208                           Bioinformatician 60%-80%  08.10.2019   \n",
       "209                          ELCA Data Science Program  24.09.2019   \n",
       "\n",
       "                                               content           location  \n",
       "0    Job descriptionInfoCompanySwiss Life Asset Man...             Zürich  \n",
       "1    Job descriptionInfoCompanyData Engineer / Data...               Bern  \n",
       "2    Job descriptionInfoCompanyData Analyst Prozess...               Bern  \n",
       "3    Job descriptionInfoCompanyProduct Data Analyst...    Rapperswil-Jona  \n",
       "4    Job descriptionInfoCompanySenior Quality Data ...           Wollerau  \n",
       "..                                                 ...                ...  \n",
       "205  Job descriptionInfoCompanyGlobal Digital Media...         Volketswil  \n",
       "206  Job descriptionInfoCompanyPostdoctoral Fellow ...              Basel  \n",
       "207  Job descriptionInfoCompanyDSAI Advanced Visual...              Basel  \n",
       "208  Job descriptionInfoCompanyBioinformatician 60%...           Lausanne  \n",
       "209  Job descriptionInfoCompanyELCA Data Science Pr...           Lausanne  \n",
       "\n",
       "[210 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BIG DATA ANALYST\n",
    "# Create the link (it is a better idea to constuct the especially link if you notice a specific pattern..)\n",
    "link_first_part = 'https://www.jobs.ch'\n",
    "link_mid_1_part = '/en/vacancies/?page='\n",
    "link_mid_2_part = '&term='\n",
    "link_mid_3_part = \"Big%20Data%20Analyst\"\n",
    "\n",
    "link = link_first_part + link_mid_1_part + link_mid_2_part + link_mid_3_part\n",
    "\n",
    "response = requests.get(link, timeout = 15)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_links = []\n",
    "def get_links(soup, job_links):\n",
    "    all_links = soup.find_all('div', class_ = 'Box-sc-7ekkso-0 Position-b2pct5-0 Position__Relative-b2pct5-1 VacancySerpItem__ShadowBox-p4qu0m-0 hthPRS')\n",
    "    for job_add in all_links:\n",
    "        job_links.append(link_first_part+job_add.find('a', {'class':'x--job-link t--job-link SearchVacancyResultsComponent__StyledVacancySerpItem-n25jij-0 dQDQbr'}).get('href'))\n",
    "\n",
    "\n",
    "\n",
    "max_pages = soup.find('div', class_ = 'Div-v2w9ke-0 Flex-sc-4aokm-0 eykbax').text.split()[2]\n",
    "\n",
    "for page in tqdm(range(1, int(max_pages)+1)[:]):\n",
    "    url = link_first_part +link_mid_1_part + str(page) + link_mid_2_part + link_mid_3_part\n",
    "    response = requests.get(url, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    get_links(soup, job_links)\n",
    "    sleep(0.6)\n",
    "\n",
    "def get_content(soup):\n",
    "    try:\n",
    "        content = soup.find('div', class_ = 'Div-v2w9ke-0 fjQgMg').get_text()\n",
    "    except:\n",
    "        content = np.nan\n",
    "    try:    \n",
    "        title = soup.find('div', {'class' : 'Div-v2w9ke-0 hPuVjT'}).find('h1').get('title')\n",
    "    except:\n",
    "        title = np.nan\n",
    "    try:\n",
    "        company = soup.find('div', class_ = 'Div-v2w9ke-0 cvwY').find('a').get('title')\n",
    "    except:\n",
    "        company = np.nan\n",
    "    try:\n",
    "        published = soup.find('span', class_ = 'Span-bhy2uh-0 Badge-ndaeev-0 krRxxu').get_text()\n",
    "    except: \n",
    "        published = np.nan\n",
    "    try:\n",
    "        location = soup.find('span', class_ = 'Span-bhy2uh-0 Text__span-sc-1vcmz87-8 YbklG Span-bhy2uh-0 Text__span-sc-1vcmz87-8 Text-sc-1vcmz87-9 gdfMMD').get_text()\n",
    "    except: \n",
    "        location = np.nan\n",
    "        \n",
    "    return content, title, company, published, location\n",
    "\n",
    "get_content(soup)\n",
    "\n",
    "cols = ['date', 'company', 'title', 'published', 'content', 'location']\n",
    "df_bigdataanalyst = pd.DataFrame(columns = cols)\n",
    "\n",
    "\n",
    "for job in tqdm(job_links[:]):\n",
    "    response = requests.get(job, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content, title, company, published, location = get_content(soup)\n",
    "    \n",
    "    df_bigdataanalyst = df_bigdataanalyst.append({\n",
    "        'date': datetime.now(), \n",
    "        'company': company, \n",
    "        'title': title, \n",
    "        'published': published, \n",
    "        'content': content,\n",
    "        'location': location\n",
    "        \n",
    "        \n",
    "    }, ignore_index = True)\n",
    "    sleep(0.6)\n",
    "\n",
    "\n",
    "df_bigdataanalyst['location'] = df_bigdataanalyst['location'].str.replace('—', '')\n",
    "df_bigdataanalyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1139a4d2f640329dced11137c06fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:60: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99737b4dad074973a72344ec14bd66c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=571), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>content</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-16 22:30:49.532482</td>\n",
       "      <td>gateB AG</td>\n",
       "      <td>Senior Consultant Data Science</td>\n",
       "      <td>24.10.2019</td>\n",
       "      <td>Job descriptionInfoCompanySenior Consultant Da...</td>\n",
       "      <td>Steinhausen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-16 22:30:51.448850</td>\n",
       "      <td>Leica Geosystems AG</td>\n",
       "      <td>Machine Learning Engineer (f/m)</td>\n",
       "      <td>09.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyMachine Learning Eng...</td>\n",
       "      <td>Heerbrugg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-16 22:30:53.386530</td>\n",
       "      <td>Arcanite Solutions Sàrl</td>\n",
       "      <td>Développeur/-euse Python orienté Web</td>\n",
       "      <td>14.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyArcanite est une jeu...</td>\n",
       "      <td>Puidoux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-16 22:30:55.391208</td>\n",
       "      <td>Noser Engineering AG</td>\n",
       "      <td>Data Engineer mit Flair für Analytics</td>\n",
       "      <td>15.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyData Engineer mit Fl...</td>\n",
       "      <td>Winterthur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-16 22:30:58.303359</td>\n",
       "      <td>Geberit AG</td>\n",
       "      <td>Product Data Analyst (m/w)</td>\n",
       "      <td>15.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyProduct Data Analyst...</td>\n",
       "      <td>Rapperswil-Jona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>2019-11-16 22:51:31.597390</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>DevOps Engineer Container Platform 80% bis 100%</td>\n",
       "      <td>04.09.2019</td>\n",
       "      <td>Job descriptionInfoDevOps Engineer Container P...</td>\n",
       "      <td>Zürich oder Bern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>2019-11-16 22:51:34.100681</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>Cloud System Engineer 60%</td>\n",
       "      <td>28.10.2019</td>\n",
       "      <td>Job descriptionInfoCloud System Engineer 60%  ...</td>\n",
       "      <td>Bern-Ittigen or Zurich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>2019-11-16 22:51:36.329481</td>\n",
       "      <td>Microsoft Schweiz GmbH</td>\n",
       "      <td>Developer Engagement Lead</td>\n",
       "      <td>08.11.2019</td>\n",
       "      <td>Job descriptionInfoCompanyDeveloper Engagement...</td>\n",
       "      <td>Wallisellen, Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>2019-11-16 22:51:38.539638</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>Senior System Engineer 80% bis 100%</td>\n",
       "      <td>16.10.2019</td>\n",
       "      <td>Job descriptionInfoSenior System Engineer 80% ...</td>\n",
       "      <td>Ittigen oder Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>2019-11-16 22:51:40.731815</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>Senior System Engineer 80% bis 100%</td>\n",
       "      <td>16.10.2019</td>\n",
       "      <td>Job descriptionInfoSenior System Engineer 80% ...</td>\n",
       "      <td>Ittigen or Zurich</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>571 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date                  company  \\\n",
       "0   2019-11-16 22:30:49.532482                 gateB AG   \n",
       "1   2019-11-16 22:30:51.448850      Leica Geosystems AG   \n",
       "2   2019-11-16 22:30:53.386530  Arcanite Solutions Sàrl   \n",
       "3   2019-11-16 22:30:55.391208     Noser Engineering AG   \n",
       "4   2019-11-16 22:30:58.303359               Geberit AG   \n",
       "..                         ...                      ...   \n",
       "566 2019-11-16 22:51:31.597390    Swisscom (Schweiz) AG   \n",
       "567 2019-11-16 22:51:34.100681    Swisscom (Schweiz) AG   \n",
       "568 2019-11-16 22:51:36.329481   Microsoft Schweiz GmbH   \n",
       "569 2019-11-16 22:51:38.539638    Swisscom (Schweiz) AG   \n",
       "570 2019-11-16 22:51:40.731815    Swisscom (Schweiz) AG   \n",
       "\n",
       "                                               title   published  \\\n",
       "0                     Senior Consultant Data Science  24.10.2019   \n",
       "1                    Machine Learning Engineer (f/m)  09.11.2019   \n",
       "2               Développeur/-euse Python orienté Web  14.11.2019   \n",
       "3              Data Engineer mit Flair für Analytics  15.11.2019   \n",
       "4                         Product Data Analyst (m/w)  15.11.2019   \n",
       "..                                               ...         ...   \n",
       "566  DevOps Engineer Container Platform 80% bis 100%  04.09.2019   \n",
       "567                        Cloud System Engineer 60%  28.10.2019   \n",
       "568                        Developer Engagement Lead  08.11.2019   \n",
       "569              Senior System Engineer 80% bis 100%  16.10.2019   \n",
       "570              Senior System Engineer 80% bis 100%  16.10.2019   \n",
       "\n",
       "                                               content  \\\n",
       "0    Job descriptionInfoCompanySenior Consultant Da...   \n",
       "1    Job descriptionInfoCompanyMachine Learning Eng...   \n",
       "2    Job descriptionInfoCompanyArcanite est une jeu...   \n",
       "3    Job descriptionInfoCompanyData Engineer mit Fl...   \n",
       "4    Job descriptionInfoCompanyProduct Data Analyst...   \n",
       "..                                                 ...   \n",
       "566  Job descriptionInfoDevOps Engineer Container P...   \n",
       "567  Job descriptionInfoCloud System Engineer 60%  ...   \n",
       "568  Job descriptionInfoCompanyDeveloper Engagement...   \n",
       "569  Job descriptionInfoSenior System Engineer 80% ...   \n",
       "570  Job descriptionInfoSenior System Engineer 80% ...   \n",
       "\n",
       "                     location  \n",
       "0                 Steinhausen  \n",
       "1                   Heerbrugg  \n",
       "2                     Puidoux  \n",
       "3                  Winterthur  \n",
       "4             Rapperswil-Jona  \n",
       "..                        ...  \n",
       "566          Zürich oder Bern  \n",
       "567    Bern-Ittigen or Zurich  \n",
       "568       Wallisellen, Zürich  \n",
       "569       Ittigen oder Zürich  \n",
       "570         Ittigen or Zurich  \n",
       "\n",
       "[571 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DATA PYTHON\n",
    "# Create the link (it is a better idea to constuct the especially link if you notice a specific pattern..)\n",
    "link_first_part = 'https://www.jobs.ch'\n",
    "link_mid_1_part = '/en/vacancies/?page='\n",
    "link_mid_2_part = '&term='\n",
    "link_mid_3_part = \"Data%20Python\"\n",
    "\n",
    "link = link_first_part + link_mid_1_part + link_mid_2_part + link_mid_3_part\n",
    "\n",
    "response = requests.get(link, timeout = 15)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_links = []\n",
    "def get_links(soup, job_links):\n",
    "    all_links = soup.find_all('div', class_ = 'Box-sc-7ekkso-0 Position-b2pct5-0 Position__Relative-b2pct5-1 VacancySerpItem__ShadowBox-p4qu0m-0 hthPRS')\n",
    "    for job_add in all_links:\n",
    "        job_links.append(link_first_part+job_add.find('a', {'class':'x--job-link t--job-link SearchVacancyResultsComponent__StyledVacancySerpItem-n25jij-0 dQDQbr'}).get('href'))\n",
    "\n",
    "\n",
    "\n",
    "max_pages = soup.find('div', class_ = 'Div-v2w9ke-0 Flex-sc-4aokm-0 eykbax').text.split()[2]\n",
    "\n",
    "for page in tqdm(range(1, int(max_pages)+1)[:]):\n",
    "    url = link_first_part +link_mid_1_part + str(page) + link_mid_2_part + link_mid_3_part\n",
    "    response = requests.get(url, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    get_links(soup, job_links)\n",
    "    sleep(0.6)\n",
    "\n",
    "def get_content(soup):\n",
    "    try:\n",
    "        content = soup.find('div', class_ = 'Div-v2w9ke-0 fjQgMg').get_text()\n",
    "    except:\n",
    "        content = np.nan\n",
    "    try:    \n",
    "        title = soup.find('div', {'class' : 'Div-v2w9ke-0 hPuVjT'}).find('h1').get('title')\n",
    "    except:\n",
    "        title = np.nan\n",
    "    try:\n",
    "        company = soup.find('div', class_ = 'Div-v2w9ke-0 cvwY').find('a').get('title')\n",
    "    except:\n",
    "        company = np.nan\n",
    "    try:\n",
    "        published = soup.find('span', class_ = 'Span-bhy2uh-0 Badge-ndaeev-0 krRxxu').get_text()\n",
    "    except: \n",
    "        published = np.nan\n",
    "    try:\n",
    "        location = soup.find('span', class_ = 'Span-bhy2uh-0 Text__span-sc-1vcmz87-8 YbklG Span-bhy2uh-0 Text__span-sc-1vcmz87-8 Text-sc-1vcmz87-9 gdfMMD').get_text()\n",
    "    except: \n",
    "        location = np.nan\n",
    "        \n",
    "    return content, title, company, published, location\n",
    "\n",
    "get_content(soup)\n",
    "\n",
    "cols = ['date', 'company', 'title', 'published', 'content', 'location']\n",
    "df_datapython = pd.DataFrame(columns = cols)\n",
    "\n",
    "\n",
    "for job in tqdm(job_links[:]):\n",
    "    response = requests.get(job, timeout = 15)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    content, title, company, published, location = get_content(soup)\n",
    "    \n",
    "    df_datapython = df_datapython.append({\n",
    "        'date': datetime.now(), \n",
    "        'company': company, \n",
    "        'title': title, \n",
    "        'published': published, \n",
    "        'content': content,\n",
    "        'location': location\n",
    "        \n",
    "        \n",
    "    }, ignore_index = True)\n",
    "    sleep(0.6)\n",
    "\n",
    "\n",
    "df_datapython['location'] = df_datapython['location'].str.replace('—', '')\n",
    "df_datapython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
